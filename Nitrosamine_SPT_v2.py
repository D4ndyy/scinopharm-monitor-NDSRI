import streamlit as st
import pandas as pd
import requests
import pdfplumber
import io
import re
import warnings
from bs4 import BeautifulSoup
import urllib3
import json

# å¿½ç•¥ SSL è­¦å‘Š
warnings.filterwarnings("ignore")
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="ScinoPharm Nitrosamine Monitor", layout="wide")
st.title(" ScinoPharm Nitrosamine Monitor (v7.8 History Tracking)")
st.markdown("""
###  v2 åŠŸèƒ½æ›´æ–°ï¼š
1.  **æ­·å²è¿½è¹¤ (Tracking)**ï¼šä¸Šå‚³ä¸Šæ¬¡çš„ Excel å ±è¡¨ï¼Œç¨‹å¼æœƒè‡ªå‹•æ¯”å°ä¸¦æ¨™è¨˜å‡ºæœ¬æ¬¡æ–°å¢çš„è³‡æ–™ (Status: â˜… NEW)ã€‚
2.  **EMA æŠ“å–ä¿®å¾© **ï¼šä¿ç•™ EMA å¤šåˆ†é è®€å–èˆ‡å¯¬é¬†è¡¨é ­åˆ¤å®šã€‚
3.  **å…¶ä»–ä¿®æ­£**ï¼šä¿ç•™ FDA æŠ“å–ã€åŒ–å­¸åŸºåœ˜éæ¿¾ç­‰åŠŸèƒ½ã€‚
""")

# ==========================================
# 0. å®šç¾©é€šç”¨å­—èˆ‡é›œè¨Š (Stop Words)
# ==========================================
STOP_WORDS = {
    "ACID", "SODIUM", "POTASSIUM", "CALCIUM", "MAGNESIUM", "HYDROCHLORIDE",
    "HCL", "HYDROBROMIDE", "HBR", "ACETATE", "TARTRATE", "CITRATE", "MALEATE",
    "FUMARATE", "MESYLATE", "SUCCINATE", "PHOSPHATE", "SULFATE", "BASE",
    "BENZOATE", "PAMOATE", "ESTOLATE", "GLUCEPTATE", "GLUCONATE", "LACTATE",
    "STEARATE", "ETHYL", "METHYL", "PROPYL", "BUTYL", "PHENYL", "BENZYL",
    "ESTER", "USP", "EP", "BP", "JP", "TABLETS", "CAPSULES", "INJECTION",
    "SOLUTION", "ORAL", "EXTENDED", "RELEASE", "API", "NAME", "PRODUCT",
    "DRUG", "SUBSTANCE", "UNKNOWN", "AND", "WITH", "FORM", "TYPE", "CLASS",
    "GRADE", "GROUP", "PART", "COMPOUND", "IMPURITY", "NEW", "NAB",
    "CHAIN", "SIDE", "FULL", "PROTECTED", "FRAGMENT"
}

# ==========================================
# 1. æ ¸å¿ƒå‡½æ•¸: ç”¢å“æ¸…å–®ä¾†æº
# ==========================================


@st.cache_data(ttl=3600)
def get_scinopharm_apis_auto():
    base_url = "https://www.scinopharm.com"
    target_url = "https://www.scinopharm.com/tw/products-detail/commercialAPI/"

    REAL_HEADERS = {
        "User-Agent":
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept":
        "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Referer": "https://www.scinopharm.com/"
    }

    product_dict = {}
    debug_logs = []

    try:
        r = requests.get(target_url,
                         headers=REAL_HEADERS,
                         verify=False,
                         timeout=15)
        soup = BeautifulSoup(r.text, 'html.parser')

        target_keywords = ["ä¸‹è¼‰ç”¢å“æ¸…å–®", "ä¸‹è¼‰è—¥ç‰©ä¸»æª”ç”³è«‹åˆ—è¡¨"]
        pdf_links = []

        for a in soup.find_all('a', href=True):
            if any(k in a.get_text(strip=True) for k in target_keywords):
                full_link = a['href']
                if not full_link.startswith("http"):
                    full_link = base_url + full_link if full_link.startswith(
                        "/") else base_url + "/" + full_link
                pdf_links.append(full_link)

        if not pdf_links:
            pdf_links.append("https://www.scinopharm.com/tw/download/43/")
            debug_logs.append("âš ï¸ æœªåœ¨é é¢æ‰¾åˆ°é€£çµï¼Œä½¿ç”¨é è¨­ ID 43 é€²è¡Œå˜—è©¦ã€‚")

        for link in pdf_links:
            debug_logs.append(f"è™•ç†é€£çµ: {link}")
            try:
                pdf_resp = requests.get(link,
                                        headers=REAL_HEADERS,
                                        verify=False,
                                        timeout=15)
                pdf_resp.raise_for_status()

                if not pdf_resp.content.startswith(b'%PDF-'):
                    debug_logs.append(f"âŒ ç•¥é: ä¸‹è¼‰å…§å®¹ä¸æ˜¯ PDF (å¯èƒ½æ˜¯ HTML éŒ¯èª¤é é¢)ã€‚")
                    continue

                debug_logs.append("âœ… æ ¼å¼é©—è­‰æˆåŠŸï¼Œé–‹å§‹è§£æ...")

                with pdfplumber.open(io.BytesIO(pdf_resp.content)) as pdf:
                    for page in pdf.pages:
                        tables = page.extract_tables()
                        found_in_table = False
                        if tables:
                            for table in tables:
                                for row in table:
                                    if row and len(row) > 0:
                                        val = str(row[0]).strip()
                                        if is_valid_api_name(val):
                                            name = clean_api_name(val)
                                            if name not in product_dict:
                                                product_dict[name] = "N/A"
                                            found_in_table = True

                        if not found_in_table:
                            text = page.extract_text()
                            if text:
                                lines = text.split('\n')
                                for line in lines:
                                    parts = re.split(r'\s{2,}', line.strip())
                                    if parts:
                                        candidate = parts[0]
                                        if is_valid_api_name(candidate):
                                            name = clean_api_name(candidate)
                                            if name not in product_dict:
                                                product_dict[name] = "N/A"

            except requests.exceptions.RequestException as e:
                debug_logs.append(f"âŒ ç¶²è·¯è«‹æ±‚å¤±æ•—: {e}")
            except Exception as e:
                debug_logs.append(f"âŒ è§£æéç¨‹éŒ¯èª¤: {e}")

    except Exception as e:
        debug_logs.append(f"âŒ åˆå§‹é€£ç·šå¤±æ•—: {e}")

    result_list = [{'name': k, 'spt': v} for k, v in product_dict.items()]
    return sorted(result_list, key=lambda x: x['name']), debug_logs


def parse_uploaded_file(uploaded_file):
    product_dict = {}
    logs = []

    try:
        if uploaded_file.name.endswith('.csv'):
            try:
                df = pd.read_csv(uploaded_file)
            except:
                uploaded_file.seek(0)
                df = pd.read_csv(uploaded_file, encoding='cp1252')
        else:
            df = pd.read_excel(uploaded_file)

        logs.append(f"ğŸ“„ è®€å–æ¬„ä½: {list(df.columns)}")

        spt_col = None
        for col in df.columns:
            if "spt" in str(col).lower():
                spt_col = col
                break

        if spt_col:
            logs.append(f"âœ… æ‰¾åˆ° SPT æ¬„ä½: '{spt_col}'")
        else:
            logs.append("âš ï¸ æœªæ‰¾åˆ°å«æœ‰ 'SPT' çš„æ¬„ä½ï¼Œå°‡é¡¯ç¤ºç‚º N/A")

        target_col = None
        target_col_2 = None
        possible_names = [
            'product', 'api', 'name', 'drug', 'item', 'substance', 'ç”¢å“', 'è—¥å',
            'å“é …'
        ]

        for col in df.columns:
            if any(p == str(col).lower() for p in possible_names):
                target_col = col
                break
        if not target_col:
            for col in df.columns:
                if any(p in str(col).lower() for p in possible_names):
                    target_col = col
                    break

        if target_col and "product" in str(target_col).lower():
            for col in df.columns:
                if str(col) != str(target_col) and "product" in str(
                        col).lower() and ("1" in str(col) or "2" in str(col)):
                    target_col_2 = col
                    break

        if not target_col:
            target_col = df.columns[0]
            logs.append(f"âš ï¸ æœªæ‰¾åˆ°æ˜ç¢ºçš„ç”¢å“æ¬„ä½ï¼Œä½¿ç”¨ç¬¬ä¸€æ¬„: '{target_col}'")
        else:
            logs.append(f"âœ… æ‰¾åˆ°ä¸»ç”¢å“æ¬„ä½: '{target_col}'")
            if target_col_2:
                logs.append(f"âœ… æ‰¾åˆ°å‰¯ç”¢å“æ¬„ä½ (å°‡åˆä½µ): '{target_col_2}'")

        for _, row in df.iterrows():
            val1 = str(row[target_col]).strip()
            name_str = val1

            if target_col_2:
                val2 = row[target_col_2]
                if pd.notna(val2) and str(val2).strip() != '' and str(
                        val2).strip().lower() != 'nan':
                    name_str = f"{val1} {str(val2).strip()}"

            if name_str.lower() == 'nan' or not name_str:
                continue

            cleaned_name = clean_api_name(name_str)

            is_generic_compound = False
            if "compound" in cleaned_name.lower():
                remain = cleaned_name.lower().replace("compound", "").strip()
                if re.fullmatch(r'[a-z0-9\s\-\.]*', remain):
                    is_generic_compound = True

            if is_generic_compound:
                continue

            if len(cleaned_name) > 2:
                spt_val = "N/A"
                if spt_col:
                    raw_spt = row[spt_col]
                    if pd.notna(raw_spt):
                        spt_val = str(raw_spt).strip()

                if cleaned_name not in product_dict:
                    product_dict[cleaned_name] = spt_val

        logs.append(f"âœ… æˆåŠŸè™•ç† {len(product_dict)} ç­†ç”¢å“è³‡æ–™ã€‚")

    except Exception as e:
        logs.append(f"âŒ æª”æ¡ˆè®€å–å¤±æ•—: {str(e)}")

    result_list = [{'name': k, 'spt': v} for k, v in product_dict.items()]
    return sorted(result_list, key=lambda x: x['name']), logs


def is_valid_api_name(text):
    if not text: return False
    text = text.lower()
    ignore = [
        "api name", "regulatory", "therapeutic", "page", "scinopharm",
        "download", "date", "status", "product"
    ]
    if any(x in text for x in ignore): return False
    if len(text) < 3: return False
    if not re.search(r'[a-zA-Z]', text): return False
    return True


def clean_api_name(text):
    text = re.sub(r'\s*\(.*?\)', '', text)
    text = text.replace('Â®', '').replace('â„¢', '').replace('*', '')
    return text.strip()


# ==========================================
# 2. çˆ¬èŸ²å‡½æ•¸: USFDA & EMA
# ==========================================
@st.cache_data(ttl=86400)
def get_fda_data():
    url = "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/cder-nitrosamine-impurity-acceptable-intake-limits"

    headers = {
        "User-Agent":
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Accept":
        "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8"
    }

    logs = []
    found_date = "N/A"

    try:
        session = requests.Session()
        r = session.get(url, headers=headers, verify=False, timeout=30)
        r.raise_for_status()
        raw_html = r.text
        
        soup = BeautifulSoup(raw_html, 'html.parser')
        text_content = soup.get_text(" ", strip=True)

        # å˜—è©¦æŠ“å– "Content current as of:"
        date_match = re.search(r"Content current as of:.*?([\d]{2}/[\d]{2}/[\d]{4})", text_content, re.IGNORECASE)
        if date_match:
            found_date = date_match.group(1).strip()
            logs.append(f"ğŸ“… FDA Updated Date Found: {found_date}")
        else:
            logs.append("âš ï¸ FDA Updated Date not found.")

        all_tables_data = []

        json_pattern = re.compile(r'data\s*:\s*(\[\s*\{.*\}\s*\])', re.DOTALL)
        matches = json_pattern.findall(raw_html)

        if matches:
            logs.append(
                f"Strategy 1 (JSON Regex): Found {len(matches)} potential JSON data blocks."
            )
            for i, match in enumerate(matches):
                try:
                    clean_match = match.strip()
                    json_data = json.loads(clean_match)
                    if isinstance(json_data, list) and len(json_data) > 0:
                        df = pd.DataFrame(json_data)
                        df = df.reset_index(drop=True)
                        all_tables_data.append(df)
                        logs.append(f"JSON Block {i} parsed: {len(df)} rows.")
                except:
                    pass

        soup = BeautifulSoup(raw_html, 'html.parser')
        tables = soup.find_all('table')

        for i, table in enumerate(tables):
            try:
                headers_list = []
                thead = table.find('thead')
                if thead:
                    headers_list = [
                        th.get_text(strip=True) for th in thead.find_all('th')
                    ]

                if not headers_list:
                    first_row = table.find('tr')
                    if first_row:
                        headers_list = [
                            td.get_text(strip=True)
                            for td in first_row.find_all(['td', 'th'])
                        ]

                if headers_list:
                    headers_list = [
                        h if h else f"Unnamed_{j}"
                        for j, h in enumerate(headers_list)
                    ]
                    seen = set()
                    new_headers = []
                    for h in headers_list:
                        c = h
                        count = 1
                        while c in seen:
                            c = f"{h}_{count}"
                            count += 1
                        seen.add(c)
                        new_headers.append(c)
                    headers_list = new_headers

                rows_data = []
                tbody = table.find('tbody')
                data_rows = tbody.find_all('tr') if tbody else table.find_all(
                    'tr')

                start_idx = 0
                if not thead and data_rows:
                    start_idx = 1

                for row in data_rows[start_idx:]:
                    cols = row.find_all('td')
                    if not cols: continue
                    rows_data.append([td.get_text(strip=True) for td in cols])

                if headers_list and rows_data:
                    max_len = len(headers_list)
                    clean_rows = []
                    for row in rows_data:
                        if len(row) < max_len:
                            row.extend([None] * (max_len - len(row)))
                        elif len(row) > max_len:
                            row = row[:max_len]
                        clean_rows.append(row)

                    df = pd.DataFrame(clean_rows, columns=headers_list)
                    df = df.reset_index(drop=True)
                    all_tables_data.append(df)
                    logs.append(
                        f"HTML Table {i} parsed successfully with {len(df)} rows."
                    )
            except Exception as e:
                logs.append(f"Manual parse failed for table {i}: {e}")

        valid_dfs = []
        for df in all_tables_data:
            df.columns = [
                str(c).strip().replace('\n', ' ') for c in df.columns
            ]

            rename_map = {}
            has_critical_data = False

            for col in df.columns:
                c_lower = col.lower()
                if any(k in c_lower for k in ['nitrosamine', 'impurity']):
                    rename_map[col] = 'Nitrosamine'
                    has_critical_data = True
                elif any(k in c_lower for k in ['limit', 'ai', 'intake']):
                    rename_map[col] = 'Limit'
                elif any(k in c_lower for k in ['note', 'comment', 'remark']):
                    rename_map[col] = 'Notes'
                elif any(k in c_lower
                         for k in ['source', 'drug', 'product', 'api']):
                    rename_map[col] = 'Source'
                elif any(k in c_lower for k in ['iupac', 'chemical']):
                    rename_map[col] = 'IUPAC'

            if has_critical_data:
                df = df.rename(columns=rename_map)
                for req_col in [
                        'Nitrosamine', 'Limit', 'Source', 'Notes', 'IUPAC'
                ]:
                    if req_col not in df.columns:
                        df[req_col] = pd.NA

                df = df.reset_index(drop=True)
                valid_dfs.append(df)

        if valid_dfs:
            target_dfs = valid_dfs[:2]
            final_df = pd.concat(target_dfs, ignore_index=True)
            final_df = final_df.reset_index(drop=True)
            final_df = final_df.reset_index(drop=True)
            return final_df, found_date, logs

        return pd.DataFrame(), found_date, logs

    except requests.exceptions.RequestException as e:
        return pd.DataFrame(), "N/A", [f"Network Error: {e}"]
    except Exception as e:
        return pd.DataFrame(), "N/A", [f"General Error: {e}"]


@st.cache_data(ttl=86400)
def get_ema_data():
    base_url = "https://www.ema.europa.eu"
    page_url = "https://www.ema.europa.eu/en/human-regulatory-overview/post-authorisation/pharmacovigilance-post-authorisation/referral-procedures-human-medicines/nitrosamine-impurities/nitrosamine-impurities-guidance-marketing-authorisation-holders"

    log_messages = []
    found_date = "N/A"

    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        r = requests.get(page_url, headers=headers, verify=False)
        soup = BeautifulSoup(r.text, 'html.parser')

        # å˜—è©¦æŠ“å– EMA æ—¥æœŸ
        # å¸¸è¦‹æ ¼å¼: "First published: 21/09/2020", "Last updated: 23/10/2023"
        # å°‹æ‰¾å«æœ‰ published æˆ– updated çš„æ–‡å­—å€å¡Š
        date_patterns = [
            r"(?:First published|Last updated|Published).*?(\d{2}/\d{2}/\d{4})",
            r"(\d{2}\s+[A-Za-z]+\s+\d{4})"
        ]

        text_content = soup.get_text(" ", strip=True)
        # ç°¡å–®éæ¿¾ä¸€ä¸‹ï¼Œåªæ‰¾ date é™„è¿‘çš„
        
        ema_date_match = None
        # å„ªå…ˆæ‰¾ "Last updated"
        last_updated_node = soup.find(string=re.compile(r"Last updated", re.IGNORECASE))
        if last_updated_node:
             parent_text = last_updated_node.parent.get_text(strip=True)
             # Extract date from this text
             m = re.search(r"(\d{2}/\d{2}/\d{4})", parent_text)
             if m:
                 found_date = m.group(1)
        
        if found_date == "N/A":
             # Fallback to general text search
             m = re.search(r"(?:Last updated|First published).*?(\d{2}/\d{2}/\d{4})", text_content, re.IGNORECASE)
             if m:
                 found_date = m.group(1)

        if found_date != "N/A":
             log_messages.append(f"ğŸ“… EMA Date Found: {found_date}")
        else:
             log_messages.append("âš ï¸ EMA Date not found.")


        target_link = None
        for a in soup.find_all('a', href=True):
            href = a['href']
            text = a.get_text(strip=True).lower()
            if "xlsx" in href and ("appendix" in text or "limit" in text):
                target_link = href
                break

        if not target_link:
            for a in soup.find_all('a', href=True):
                if "xlsx" in a['href']:
                    target_link = a['href']
                    break

        if target_link:
            if not target_link.startswith("http"):
                target_link = base_url + target_link

            file_resp = requests.get(target_link,
                                     headers=headers,
                                     verify=False)

            xls = pd.read_excel(io.BytesIO(file_resp.content),
                                sheet_name=None,
                                header=None)

            all_sheets_data = []

            for sheet_name, temp_df in xls.items():
                log_messages.append(f"Analyzing EMA Sheet: {sheet_name}")

                best_idx = 0
                max_score = 0
                keywords = [
                    "nitrosamine", "limit", "intake", "substance", "ng/day",
                    "iupac", "impurity", "structure", "cas", "source",
                    "ai (ng/day)"
                ]

                scan_rows = min(30, len(temp_df))
                for idx in range(scan_rows):
                    row = temp_df.iloc[idx]
                    row_text = " ".join(
                        [str(x).lower() for x in row if pd.notna(x)])
                    score = sum(1 for k in keywords if k in row_text)
                    if score > max_score:
                        max_score = score
                        best_idx = idx

                if max_score == 0 and len(temp_df) < 5:
                    log_messages.append(
                        f"  -> Skipping small/irrelevant sheet: {sheet_name}")
                    continue

                new_header = temp_df.iloc[best_idx]
                df = temp_df.iloc[best_idx + 1:].copy()
                df.columns = new_header
                df.columns = [
                    str(c).strip().replace('\n', ' ') for c in df.columns
                ]

                df = df.reset_index(drop=True)
                all_sheets_data.append(df)
                log_messages.append(
                    f"  -> Added table from {sheet_name} with {len(df)} rows.")

            if all_sheets_data:
                final_df = pd.concat(all_sheets_data, ignore_index=True)
                final_df = final_df.reset_index(drop=True)
                return final_df, found_date, log_messages

            return pd.DataFrame(), found_date, log_messages

        return pd.DataFrame(), found_date, ["No link found"]
    except Exception as e:
        return pd.DataFrame(), "N/A", [str(e)]


# ==========================================
# 3. æ ¸å¿ƒæ¯”å°é‚è¼¯ (Smart Match)
# ==========================================
def smart_match(scino_api, row_series):
    scino_clean = scino_api.upper().replace("-", " ").strip()
    scino_tokens = set(scino_clean.split())
    core_tokens = {
        t
        for t in scino_tokens if t not in STOP_WORDS and len(t) > 2
    }

    if not core_tokens:
        if "COMPOUND" in scino_clean:
            return False, ""
        core_tokens = {scino_clean}

    row_text = " ".join(
        [str(val).upper() for val in row_series.values if pd.notna(val)])

    for token in core_tokens:
        pattern = r'\b' + re.escape(token) + r'\b'
        if re.search(pattern, row_text):
            return True, row_text

    return False, ""


def get_display_col(df_columns, keyword_list):
    if isinstance(keyword_list, str):
        keyword_list = [keyword_list]

    cols = {c.lower(): c for c in df_columns}

    for kw in keyword_list:
        kw = kw.lower()
        if kw == 'name':
            for c_lower, c_orig in cols.items():
                if c_lower == 'name':
                    return c_orig

        for c_lower, c_orig in cols.items():
            if kw in c_lower:
                return c_orig
    return None


# ==========================================
# 4. Excel ç”Ÿæˆ
# ==========================================
def generate_excel(match_df, fda_raw, ema_raw):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        match_df.to_excel(writer, sheet_name='Summary_Match', index=False)
        fda_raw.to_excel(writer, sheet_name='Raw_FDA_Data', index=False)
        ema_raw.to_excel(writer, sheet_name='Raw_EMA_Data', index=False)

        workbook = writer.book
        for sheet in writer.sheets.values():
            sheet.set_column(0, 9, 20)

    return output.getvalue()


# ==========================================
# ä¸»ç¨‹å¼ UI
# ==========================================

# --- Sidebar: é¸æ“‡è³‡æ–™ä¾†æº ---
st.sidebar.header("âš™ï¸ è¨­å®š (Settings)")
source_mode = st.sidebar.radio(
    "é¸æ“‡ç”¢å“æ¸…å–®ä¾†æº (Source):",
    ("ğŸŒ è‡ªå‹•çˆ¬å–ç¥éš†å®˜ç¶² (Auto-Scrape)", "ğŸ“‚ æ‰‹å‹•ä¸Šå‚³æ¸…å–® (Manual Upload)"))

# ã€æ–°å¢åŠŸèƒ½ v7.8ã€‘æ­·å²æ¯”å°æª”æ¡ˆä¸Šå‚³
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ“œ æ­·å²è¿½è¹¤ (History Tracking)")
history_file = st.sidebar.file_uploader("ä¸Šå‚³ä¸Šæ¬¡çš„çµæœ (Optional)", type=['xlsx'])

api_list = []
log_msgs = []
ready_to_run = False

if source_mode == "ğŸŒ è‡ªå‹•çˆ¬å–ç¥éš†å®˜ç¶² (Auto-Scrape)":
    st.sidebar.info("ç¨‹å¼å°‡è‡ªå‹•é€£ç·šè‡³ scinopharm.com ä¸‹è¼‰æœ€æ–°çš„ PDF ç”¢å“åˆ—è¡¨ã€‚")
    if st.sidebar.button("è¼‰å…¥å®˜ç¶²è³‡æ–™", type="primary"):
        with st.spinner("æ­£åœ¨é€£ç·šè‡³ç¥éš†å®˜ç¶²..."):
            api_list, log_msgs = get_scinopharm_apis_auto()
            if api_list:
                st.session_state['api_list'] = api_list
                st.session_state['log_msgs'] = log_msgs
                st.success(f"æˆåŠŸè¼‰å…¥ {len(api_list)} ç­†ç”¢å“ï¼")
            else:
                st.error("æœªæ‰¾åˆ°ç”¢å“ï¼Œè«‹æª¢æŸ¥é€£ç·šæˆ–æ”¹ç”¨æ‰‹å‹•ä¸Šå‚³ã€‚")

    if 'api_list' in st.session_state and st.session_state['api_list']:
        api_list = st.session_state['api_list']
        log_msgs = st.session_state['log_msgs']
        ready_to_run = True

else:
    st.sidebar.info("è«‹ä¸Šå‚³ Excel (.xlsx) æˆ– CSV æª”ã€‚æ”¯æ´ 'SPT' æ¬„ä½è‡ªå‹•è®€å–ã€‚")
    uploaded_file = st.sidebar.file_uploader("ä¸Šå‚³ç”¢å“æ¸…å–®", type=['xlsx', 'csv'])

    if uploaded_file:
        api_list, log_msgs = parse_uploaded_file(uploaded_file)
        if api_list:
            st.sidebar.success(f"âœ… å·²è®€å– {len(api_list)} ç­†è³‡æ–™")
            ready_to_run = True
            with st.expander("é è¦½åŒ¯å…¥æ¸…å–® (å‰ 5 ç­†)"):
                st.write(api_list[:5])
        else:
            st.sidebar.error("âŒ ç„¡æ³•è®€å–è³‡æ–™ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆæ ¼å¼ã€‚")

# --- ä¸»ç•«é¢ ---

if ready_to_run:
    st.subheader(
        f"ç›®å‰ç›£æ§æ¸…å–®: {len(api_list)} é …ç”¢å“ ({'è‡ªå‹•çˆ¬å–' if source_mode.startswith('ğŸŒ') else 'æ‰‹å‹•åŒ¯å…¥'})"
    )

    if st.button("ğŸš€ é–‹å§‹åŸ·è¡Œæ¯”å° (Start Analysis)", type="primary"):
        status_box = st.status("æ­£åœ¨åˆ†æä¸­...", expanded=True)

        # 2. FDA / EMA
        status_box.write("ğŸŒ ä¸‹è¼‰ FDA / EMA è³‡æ–™åº«...")
        fda_df, fda_date, fda_logs = get_fda_data()
        ema_df, ema_date, ema_logs = get_ema_data()

        if not fda_df.empty:
            status_box.write(
                f"âœ… FDA: {len(fda_df)} ç­† (å·²éæ¿¾åƒ… Table 1 & 2), EMA: {len(ema_df)} ç­†"
            )
        else:
            status_box.write(f"âš ï¸ FDA: 0 ç­† (æŠ“å–å¤±æ•—), EMA: {len(ema_df)} ç­†")
            log_msgs.extend(fda_logs)

        # 3. æ¯”å°
        status_box.write("ğŸ” åŸ·è¡Œæ¯”å°...")
        match_results = []

        # --- FDA æ¯”å° ---
        if not fda_df.empty:
            nitro_col = get_display_col(
                fda_df.columns, ['Nitrosamine', 'nitrosamine', 'impurity'])
            limit_col = get_display_col(fda_df.columns,
                                        ['Limit', 'limit', 'ai'])
            iupac_col = get_display_col(fda_df.columns, ['IUPAC', 'iupac'])
            source_col = get_display_col(fda_df.columns, ['Source', 'source'])
            note_col = get_display_col(fda_df.columns,
                                       ['Notes', 'note', 'comment'])

            ref_col = source_col

            for _, row in fda_df.iterrows():
                for my_api_obj in api_list:
                    my_api_name = my_api_obj['name']
                    my_api_spt = my_api_obj['spt']

                    is_match, _ = smart_match(my_api_name, row)
                    if is_match:
                        match_results.append({
                            "Source":
                            "USFDA",
                            "ScinoPharm Product":
                            my_api_name,
                            "SPT Project num":
                            my_api_spt,
                            "Nitrosamine Impurity":
                            row[nitro_col] if nitro_col else "Check Row",
                            "IUPAC Name":
                            row[iupac_col] if iupac_col else "N/A",
                            "Limit (AI)":
                            row[limit_col] if limit_col else "N/A",
                            "Notes":
                            row[note_col] if note_col else "N/A",
                            "Updated date": fda_date,
                            "Matched in Column":
                            ref_col if ref_col else "Full Row Match",
                            "Reference Value":
                            row[ref_col] if ref_col else "See Raw Data"
                        })

        # --- EMA æ¯”å° ---
        if not ema_df.empty:
            nitro_col = get_display_col(ema_df.columns,
                                        ['name', 'nitrosamine', 'impurity'])
            limit_col = get_display_col(
                ema_df.columns, ['ai (ng/day)', 'limit', 'intake', 'ai'])
            iupac_col = get_display_col(ema_df.columns,
                                        ['iupac', 'chemical name'])
            source_col = get_display_col(ema_df.columns, ['source'])
            drug_col = get_display_col(
                ema_df.columns, ['substance', 'api', 'product', 'active'])
            note_col = get_display_col(ema_df.columns,
                                       ['note', 'comment', 'remark'])
            ref_col = source_col if source_col else drug_col

            for _, row in ema_df.iterrows():
                for my_api_obj in api_list:
                    my_api_name = my_api_obj['name']
                    my_api_spt = my_api_obj['spt']

                    is_match, _ = smart_match(my_api_name, row)
                    if is_match:
                        match_results.append({
                            "Source":
                            "EMA",
                            "ScinoPharm Product":
                            my_api_name,
                            "SPT Project num":
                            my_api_spt,
                            "Nitrosamine Impurity":
                            row[nitro_col] if nitro_col
                            and pd.notna(row[nitro_col]) else "Check Row",
                            "IUPAC Name":
                            row[iupac_col] if iupac_col
                            and pd.notna(row[iupac_col]) else "N/A",
                            "Limit (AI)":
                            row[limit_col] if limit_col else "N/A",
                            "Notes":
                            row[note_col]
                            if note_col and pd.notna(row[note_col]) else "N/A",
                            "Updated date": ema_date,
                            "Matched in Column":
                            ref_col if ref_col else "Full Row Match",
                            "Reference Value":
                            row[ref_col] if ref_col else "See Raw Data"
                        })

        status_box.update(label="åŸ·è¡Œå®Œæˆï¼", state="complete", expanded=False)

        # --- çµæœé¡¯ç¤º ---
        st.divider()

        if match_results:
            final_df = pd.DataFrame(match_results).drop_duplicates()

            # ã€æ–°å¢åŠŸèƒ½ v7.8ã€‘æ­·å²æ¯”å°é‚è¼¯
            final_df['Status'] = ""  # é è¨­ç‚ºç©º

            if history_file:
                try:
                    # è®€å–èˆŠæª”æ¡ˆ (é è¨­è®€å– Summary_Match åˆ†é ï¼Œè‹¥ç„¡å‰‡è®€ç¬¬ä¸€é )
                    try:
                        old_df = pd.read_excel(history_file,
                                               sheet_name='Summary_Match')
                    except:
                        old_df = pd.read_excel(history_file)

                    # å»ºç«‹æŒ‡ç´‹é›†åˆ: SPTç·¨è™Ÿ + é›œè³ªåç¨± (å»é™¤ç©ºç™½èˆ‡å¤§å°å¯«ä»¥ç¢ºä¿æ¯”å°æº–ç¢º)
                    # å¦‚æœæ²’æœ‰ SPT æ¬„ä½ï¼Œå‰‡æ”¹ç”¨ ç”¢å“åç¨± + é›œè³ªåç¨±
                    old_fingerprints = set()

                    spt_col_name = None
                    for c in old_df.columns:
                        if 'spt' in c.lower():
                            spt_col_name = c
                            break

                    nitro_col_name = None
                    for c in old_df.columns:
                        if 'nitrosamine' in c.lower(
                        ) and 'impurity' in c.lower():
                            nitro_col_name = c
                            break

                    if nitro_col_name:
                        for _, row in old_df.iterrows():
                            # çµ„åˆæŒ‡ç´‹ Key
                            key_part1 = str(row[spt_col_name]).strip().upper(
                            ) if spt_col_name else str(row[0]).strip().upper()
                            key_part2 = str(
                                row[nitro_col_name]).strip().upper()
                            old_fingerprints.add(f"{key_part1}|{key_part2}")

                    # æ¯”å°æ–°è³‡æ–™
                    new_count = 0
                    for idx, row in final_df.iterrows():
                        key_part1 = str(row['SPT Project num']).strip().upper(
                        ) if 'SPT Project num' in row else str(
                            row['ScinoPharm Product']).strip().upper()
                        key_part2 = str(
                            row['Nitrosamine Impurity']).strip().upper()
                        current_fp = f"{key_part1}|{key_part2}"

                        if current_fp not in old_fingerprints:
                            final_df.at[idx, 'Status'] = "â˜… NEW"
                            new_count += 1

                    if new_count > 0:
                        st.warning(f"ğŸ”” ç™¼ç¾ {new_count} ç­†æ–°è³‡æ–™ï¼å·²æ¨™è¨˜ç‚º 'â˜… NEW'")
                    else:
                        st.info("âœ… èˆ‡æ­·å²ç´€éŒ„ç›¸æ¯”ï¼Œç„¡æ–°å¢è³‡æ–™ã€‚")

                except Exception as e:
                    st.error(f"æ­·å²æª”æ¡ˆæ¯”å°å¤±æ•—: {e}")

            # èª¿æ•´æ¬„ä½é †åº (Status æ”¾æœ€å‰)
            cols_order = [
                "Status", "Source", "ScinoPharm Product", "SPT Project num",
                "Nitrosamine Impurity", "IUPAC Name", "Limit (AI)", "Notes",
                "Updated date", "Reference Value"
            ]
            cols_order = [c for c in cols_order if c in final_df.columns]
            final_df = final_df[cols_order]

            # æ ¹æ“š Status æ’åºï¼Œæ–°ç™¼ç¾çš„æ”¾å‰é¢
            final_df = final_df.sort_values(
                by=['Status', 'ScinoPharm Product'], ascending=[False, True])

            st.subheader(f"ğŸ“Š æ¯”å°çµæœ (å…± {len(final_df)} ç­†)")

            # ä½¿ç”¨ style highlight æ–°è³‡æ–™
            def highlight_new(row):
                return ['background-color: #ffffcc'] * len(
                    row) if row['Status'] == 'â˜… NEW' else [''] * len(row)

            st.dataframe(final_df.style.apply(highlight_new, axis=1),
                         use_container_width=True,
                         height=500)

            excel_data = generate_excel(final_df, fda_df, ema_df)
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰å®Œæ•´ Excel å ±è¡¨",
                data=excel_data,
                file_name='ScinoPharm_Nitrosamine_Analysis_v7.8.xlsx',
                mime=
                'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                type="primary")
        else:
            st.warning("âš ï¸ æ²’æœ‰æ¯”å°åˆ°çµæœã€‚")
else:
    st.info("ğŸ‘ˆ è«‹åœ¨å·¦å´å´é‚Šæ¬„é¸æ“‡è³‡æ–™ä¾†æºä¸¦è¼‰å…¥è³‡æ–™ã€‚")

# --- Debug Logs ---
with st.expander("ğŸ› ï¸ Debug Logs"):
    if log_msgs:
        for msg in log_msgs:
            st.text(msg)
    else:
        st.text("å°šç„¡ç´€éŒ„")


import streamlit as st
import pandas as pd
import requests
import pdfplumber
import io
import re
import warnings
from bs4 import BeautifulSoup
import urllib3
import json

# å¿½ç•¥ SSL è­¦å‘Š
warnings.filterwarnings("ignore")
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="ScinoPharm Nitrosamine Monitor", layout="wide")
st.title("ğŸ§ª ScinoPharm Nitrosamine Monitor (v7.6 Expanded Stop Words)")
st.markdown("""
### ğŸ› ï¸ v7.6 åŠŸèƒ½æ›´æ–°ï¼š
1.  **åœç”¨è©å¤§å¹…æ“´å……**ï¼šåŠ å…¥æ›´å¤šå¸¸è¦‹åŒ–å­¸åŸºåœ˜ (Isopropyl, Methoxy...) èˆ‡æ°´åˆç‰©æè¿° (Hydrate, Anhydrous)ï¼Œé€²ä¸€æ­¥é™ä½èª¤åˆ¤ã€‚
2.  **FDA è³‡æ–™é–å®š**ï¼šç¶­æŒåƒ…æŠ“å– FDA Table 1 & 2 çš„é‚è¼¯ã€‚
3.  **JSON å¼·åŠ›æŠ“å–**ï¼šä¿ç•™é‡å°å‹•æ…‹ç¶²é çš„è§£æèƒ½åŠ›ã€‚
""")

# ==========================================
# 0. å®šç¾©é€šç”¨å­—èˆ‡é›œè¨Š (Stop Words)
# ==========================================
STOP_WORDS = {
    # --- é¹½é¡èˆ‡é…¸æ ¹ (Salts & Acids) ---
    "ACID",
    "SODIUM",
    "POTASSIUM",
    "CALCIUM",
    "MAGNESIUM",
    "HYDROCHLORIDE",
    "HCL",
    "HYDROBROMIDE",
    "HBR",
    "ACETATE",
    "TARTRATE",
    "CITRATE",
    "MALEATE",
    "FUMARATE",
    "MESYLATE",
    "SUCCINATE",
    "PHOSPHATE",
    "SULFATE",
    "BASE",
    "BENZOATE",
    "PAMOATE",
    "ESTOLATE",
    "GLUCEPTATE",
    "GLUCONATE",
    "LACTATE",
    "STEARATE",
    "BESYLATE",
    "TOSYLATE",

    # --- å¸¸è¦‹åŒ–å­¸åŸºåœ˜èˆ‡å–ä»£åŸº (Substituents & Groups) ---
    "ETHYL",
    "METHYL",
    "PROPYL",
    "BUTYL",
    "PHENYL",
    "BENZYL",
    "ESTER",
    "ISOPROPYL",
    "TERT-BUTYL",
    "DIMETHYL",
    "DIETHYL",
    "TRIMETHYL",
    "TRIETHYL",
    "METHOXY",
    "ETHOXY",
    "PHENOXY",
    "BENZOXY",
    "HYDROXY",
    "AMINO",
    "CHLORO",
    "FLUORO",
    "BROMO",
    "IODO",
    "CYANO",
    "NITRO",
    "ACETYL",
    "BENZOYL",
    "AMIDE",
    "AMINE",
    "ETHER",
    "KETONE",

    # --- æ°´åˆç‰©èˆ‡ç‹€æ…‹ (Hydrates & State) ---
    "HYDRATE",
    "ANHYDROUS",
    "HEMIHYDRATE",
    "DIHYDRATE",
    "TRIHYDRATE",
    "SOLVATE",

    # --- è—¥å…¸èˆ‡æ¨™æº– (Pharmacopeia) ---
    "USP",
    "EP",
    "BP",
    "JP",
    "NF",

    # --- åŠ‘å‹èˆ‡ä¸€èˆ¬æè¿° (Dosage & Desc) ---
    "TABLETS",
    "CAPSULES",
    "INJECTION",
    "SOLUTION",
    "ORAL",
    "EXTENDED",
    "RELEASE",
    "API",
    "NAME",
    "PRODUCT",
    "DRUG",
    "SUBSTANCE",
    "UNKNOWN",
    "AND",
    "WITH",
    "FORM",
    "TYPE",
    "CLASS",
    "GRADE",
    "GROUP",
    "PART",
    "COMPOUND",
    "IMPURITY",
    "NEW",
    "OLD",
    "TEST",
    "SAMPLE",
    "ITEM",
    "MATERIAL",
    "DEGRADANT"
}

# ==========================================
# 1. æ ¸å¿ƒå‡½æ•¸: ç”¢å“æ¸…å–®ä¾†æº (è‡ªå‹•çˆ¬èŸ² OR æ‰‹å‹•ä¸Šå‚³)
# ==========================================


@st.cache_data(ttl=3600)
def get_scinopharm_apis_auto():
    base_url = "https://www.scinopharm.com"
    target_url = "https://www.scinopharm.com/tw/products-detail/commercialAPI/"

    REAL_HEADERS = {
        "User-Agent":
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept":
        "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Referer": "https://www.scinopharm.com/"
    }

    product_dict = {}
    debug_logs = []

    try:
        r = requests.get(target_url,
                         headers=REAL_HEADERS,
                         verify=False,
                         timeout=15)
        soup = BeautifulSoup(r.text, 'html.parser')

        target_keywords = ["ä¸‹è¼‰ç”¢å“æ¸…å–®", "ä¸‹è¼‰è—¥ç‰©ä¸»æª”ç”³è«‹åˆ—è¡¨"]
        pdf_links = []

        for a in soup.find_all('a', href=True):
            if any(k in a.get_text(strip=True) for k in target_keywords):
                full_link = a['href']
                if not full_link.startswith("http"):
                    full_link = base_url + full_link if full_link.startswith(
                        "/") else base_url + "/" + full_link
                pdf_links.append(full_link)

        if not pdf_links:
            pdf_links.append("https://www.scinopharm.com/tw/download/43/")
            debug_logs.append("âš ï¸ æœªåœ¨é é¢æ‰¾åˆ°é€£çµï¼Œä½¿ç”¨é è¨­ ID 43 é€²è¡Œå˜—è©¦ã€‚")

        for link in pdf_links:
            debug_logs.append(f"è™•ç†é€£çµ: {link}")
            try:
                pdf_resp = requests.get(link,
                                        headers=REAL_HEADERS,
                                        verify=False,
                                        timeout=15)
                pdf_resp.raise_for_status()

                if not pdf_resp.content.startswith(b'%PDF-'):
                    debug_logs.append(f"âŒ ç•¥é: ä¸‹è¼‰å…§å®¹ä¸æ˜¯ PDF (å¯èƒ½æ˜¯ HTML éŒ¯èª¤é é¢)ã€‚")
                    continue

                debug_logs.append("âœ… æ ¼å¼é©—è­‰æˆåŠŸï¼Œé–‹å§‹è§£æ...")

                with pdfplumber.open(io.BytesIO(pdf_resp.content)) as pdf:
                    for page in pdf.pages:
                        tables = page.extract_tables()
                        found_in_table = False
                        if tables:
                            for table in tables:
                                for row in table:
                                    if row and len(row) > 0:
                                        val = str(row[0]).strip()
                                        if is_valid_api_name(val):
                                            name = clean_api_name(val)
                                            if name not in product_dict:
                                                product_dict[name] = "N/A"
                                            found_in_table = True

                        if not found_in_table:
                            text = page.extract_text()
                            if text:
                                lines = text.split('\n')
                                for line in lines:
                                    parts = re.split(r'\s{2,}', line.strip())
                                    if parts:
                                        candidate = parts[0]
                                        if is_valid_api_name(candidate):
                                            name = clean_api_name(candidate)
                                            if name not in product_dict:
                                                product_dict[name] = "N/A"

            except requests.exceptions.RequestException as e:
                debug_logs.append(f"âŒ ç¶²è·¯è«‹æ±‚å¤±æ•—: {e}")
            except Exception as e:
                debug_logs.append(f"âŒ è§£æéç¨‹éŒ¯èª¤: {e}")

    except Exception as e:
        debug_logs.append(f"âŒ åˆå§‹é€£ç·šå¤±æ•—: {e}")

    result_list = [{'name': k, 'spt': v} for k, v in product_dict.items()]
    return sorted(result_list, key=lambda x: x['name']), debug_logs


def parse_uploaded_file(uploaded_file):
    product_dict = {}
    logs = []

    try:
        if uploaded_file.name.endswith('.csv'):
            try:
                df = pd.read_csv(uploaded_file)
            except:
                uploaded_file.seek(0)
                df = pd.read_csv(uploaded_file, encoding='cp1252')
        else:
            df = pd.read_excel(uploaded_file)

        logs.append(f"ğŸ“„ è®€å–æ¬„ä½: {list(df.columns)}")

        spt_col = None
        for col in df.columns:
            if "spt" in str(col).lower():
                spt_col = col
                break

        if spt_col:
            logs.append(f"âœ… æ‰¾åˆ° SPT æ¬„ä½: '{spt_col}'")
        else:
            logs.append("âš ï¸ æœªæ‰¾åˆ°å«æœ‰ 'SPT' çš„æ¬„ä½ï¼Œå°‡é¡¯ç¤ºç‚º N/A")

        target_col = None
        target_col_2 = None
        possible_names = [
            'product', 'api', 'name', 'drug', 'item', 'substance', 'ç”¢å“', 'è—¥å',
            'å“é …'
        ]

        for col in df.columns:
            if any(p == str(col).lower() for p in possible_names):
                target_col = col
                break
        if not target_col:
            for col in df.columns:
                if any(p in str(col).lower() for p in possible_names):
                    target_col = col
                    break

        if target_col and "product" in str(target_col).lower():
            for col in df.columns:
                if str(col) != str(target_col) and "product" in str(
                        col).lower() and ("1" in str(col) or "2" in str(col)):
                    target_col_2 = col
                    break

        if not target_col:
            target_col = df.columns[0]
            logs.append(f"âš ï¸ æœªæ‰¾åˆ°æ˜ç¢ºçš„ç”¢å“æ¬„ä½ï¼Œä½¿ç”¨ç¬¬ä¸€æ¬„: '{target_col}'")
        else:
            logs.append(f"âœ… æ‰¾åˆ°ä¸»ç”¢å“æ¬„ä½: '{target_col}'")
            if target_col_2:
                logs.append(f"âœ… æ‰¾åˆ°å‰¯ç”¢å“æ¬„ä½ (å°‡åˆä½µ): '{target_col_2}'")

        for _, row in df.iterrows():
            val1 = str(row[target_col]).strip()
            name_str = val1

            if target_col_2:
                val2 = row[target_col_2]
                if pd.notna(val2) and str(val2).strip() != '' and str(
                        val2).strip().lower() != 'nan':
                    name_str = f"{val1} {str(val2).strip()}"

            if name_str.lower() == 'nan' or not name_str:
                continue

            cleaned_name = clean_api_name(name_str)

            # éæ¿¾ "Compound X"
            is_generic_compound = False
            if "compound" in cleaned_name.lower():
                remain = cleaned_name.lower().replace("compound", "").strip()
                if re.fullmatch(r'[a-z0-9\s\-\.]*', remain):
                    is_generic_compound = True

            if is_generic_compound:
                continue

            if len(cleaned_name) > 2:
                spt_val = "N/A"
                if spt_col:
                    raw_spt = row[spt_col]
                    if pd.notna(raw_spt):
                        spt_val = str(raw_spt).strip()

                if cleaned_name not in product_dict:
                    product_dict[cleaned_name] = spt_val

        logs.append(f"âœ… æˆåŠŸè™•ç† {len(product_dict)} ç­†ç”¢å“è³‡æ–™ã€‚")

    except Exception as e:
        logs.append(f"âŒ æª”æ¡ˆè®€å–å¤±æ•—: {str(e)}")

    result_list = [{'name': k, 'spt': v} for k, v in product_dict.items()]
    return sorted(result_list, key=lambda x: x['name']), logs


def is_valid_api_name(text):
    if not text: return False
    text = text.lower()
    ignore = [
        "api name", "regulatory", "therapeutic", "page", "scinopharm",
        "download", "date", "status", "product"
    ]
    if any(x in text for x in ignore): return False
    if len(text) < 3: return False
    if not re.search(r'[a-zA-Z]', text): return False
    return True


def clean_api_name(text):
    text = re.sub(r'\s*\(.*?\)', '', text)
    text = text.replace('Â®', '').replace('â„¢', '').replace('*', '')
    return text.strip()


# ==========================================
# 2. çˆ¬èŸ²å‡½æ•¸: USFDA & EMA
# ==========================================
@st.cache_data(ttl=86400)
def get_fda_data():
    url = "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/cder-nitrosamine-impurity-acceptable-intake-limits"

    headers = {
        "User-Agent":
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Accept":
        "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8"
    }

    logs = []

    try:
        session = requests.Session()
        r = session.get(url, headers=headers, verify=False, timeout=30)
        r.raise_for_status()
        raw_html = r.text

        all_tables_data = []

        # --- ç­–ç•¥ 1: å˜—è©¦å¾ JSON è³‡æ–™ä¸­æå– (é‡å°å‹•æ…‹è¡¨æ ¼) ---
        json_pattern = re.compile(r'data\s*:\s*(\[\s*\{.*\}\s*\])', re.DOTALL)
        matches = json_pattern.findall(raw_html)

        if matches:
            logs.append(
                f"Strategy 1 (JSON Regex): Found {len(matches)} potential JSON data blocks."
            )
            for i, match in enumerate(matches):
                try:
                    clean_match = match.strip()
                    json_data = json.loads(clean_match)
                    if isinstance(json_data, list) and len(json_data) > 0:
                        df = pd.DataFrame(json_data)
                        all_tables_data.append(df)
                        logs.append(f"JSON Block {i} parsed: {len(df)} rows.")
                except:
                    pass

        # --- ç­–ç•¥ 2: æ‰‹å‹•è§£æ HTML Table (å‚™æ´) ---
        soup = BeautifulSoup(raw_html, 'html.parser')
        tables = soup.find_all('table')

        for i, table in enumerate(tables):
            try:
                headers_list = []
                thead = table.find('thead')
                if thead:
                    headers_list = [
                        th.get_text(strip=True) for th in thead.find_all('th')
                    ]

                if not headers_list:
                    first_row = table.find('tr')
                    if first_row:
                        headers_list = [
                            td.get_text(strip=True)
                            for td in first_row.find_all(['td', 'th'])
                        ]

                # é˜²å‘†ï¼šç¢ºä¿ headers ä¸é‡è¤‡
                if headers_list:
                    headers_list = [
                        h if h else f"Unnamed_{j}"
                        for j, h in enumerate(headers_list)
                    ]
                    seen = set()
                    new_headers = []
                    for h in headers_list:
                        c = h
                        count = 1
                        while c in seen:
                            c = f"{h}_{count}"
                            count += 1
                        seen.add(c)
                        new_headers.append(c)
                    headers_list = new_headers

                rows_data = []
                tbody = table.find('tbody')
                data_rows = tbody.find_all('tr') if tbody else table.find_all(
                    'tr')

                start_idx = 0
                if not thead and data_rows:
                    start_idx = 1

                for row in data_rows[start_idx:]:
                    cols = row.find_all('td')
                    if not cols: continue
                    rows_data.append([td.get_text(strip=True) for td in cols])

                if headers_list and rows_data:
                    max_len = len(headers_list)
                    clean_rows = []
                    for row in rows_data:
                        if len(row) < max_len:
                            row.extend([None] * (max_len - len(row)))
                        elif len(row) > max_len:
                            row = row[:max_len]
                        clean_rows.append(row)

                    df = pd.DataFrame(clean_rows, columns=headers_list)
                    df = df.reset_index(drop=True)
                    all_tables_data.append(df)
                    logs.append(
                        f"HTML Table {i} parsed successfully with {len(df)} rows."
                    )
            except Exception as e:
                logs.append(f"Manual parse failed for table {i}: {e}")

        # --- è™•ç†èˆ‡æ¨™æº–åŒ– ---
        valid_dfs = []
        for df in all_tables_data:
            # æ¸…ç†æ¬„ä½
            df.columns = [
                str(c).strip().replace('\n', ' ') for c in df.columns
            ]

            rename_map = {}
            has_critical_data = False

            for col in df.columns:
                c_lower = col.lower()
                if any(k in c_lower for k in ['nitrosamine', 'impurity']):
                    rename_map[col] = 'Nitrosamine'
                    has_critical_data = True
                elif any(k in c_lower for k in ['limit', 'ai', 'intake']):
                    rename_map[col] = 'Limit'
                elif any(k in c_lower for k in ['note', 'comment', 'remark']):
                    rename_map[col] = 'Notes'
                elif any(k in c_lower
                         for k in ['source', 'drug', 'product', 'api']):
                    rename_map[col] = 'Source'
                elif any(k in c_lower for k in ['iupac', 'chemical']):
                    rename_map[col] = 'IUPAC'

            if has_critical_data:
                df = df.rename(columns=rename_map)
                for req_col in [
                        'Nitrosamine', 'Limit', 'Source', 'Notes', 'IUPAC'
                ]:
                    if req_col not in df.columns:
                        df[req_col] = pd.NA

                df = df.reset_index(drop=True)
                valid_dfs.append(df)

        # ã€é—œéµä¿®æ­£ã€‘åªå–å‰å…©å€‹è¡¨æ ¼ (Table 1 & 2)
        if valid_dfs:
            target_dfs = valid_dfs[:2]
            final_df = pd.concat(target_dfs, ignore_index=True)
            final_df = final_df.reset_index(drop=True)
            return final_df, logs

        return pd.DataFrame(), logs

    except requests.exceptions.RequestException as e:
        return pd.DataFrame(), [f"Network Error: {e}"]
    except Exception as e:
        return pd.DataFrame(), [f"General Error: {e}"]


@st.cache_data(ttl=86400)
def get_ema_data():
    base_url = "https://www.ema.europa.eu"
    page_url = "https://www.ema.europa.eu/en/human-regulatory-overview/post-authorisation/pharmacovigilance-post-authorisation/referral-procedures-human-medicines/nitrosamine-impurities/nitrosamine-impurities-guidance-marketing-authorisation-holders"

    log_messages = []

    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        r = requests.get(page_url, headers=headers, verify=False)
        soup = BeautifulSoup(r.text, 'html.parser')

        target_link = None
        for a in soup.find_all('a', href=True):
            href = a['href']
            text = a.get_text(strip=True).lower()
            if "xlsx" in href and ("appendix" in text or "limit" in text):
                target_link = href
                break

        if not target_link:
            for a in soup.find_all('a', href=True):
                if "xlsx" in a['href']:
                    target_link = a['href']
                    break

        if target_link:
            if not target_link.startswith("http"):
                target_link = base_url + target_link

            file_resp = requests.get(target_link,
                                     headers=headers,
                                     verify=False)

            temp_df = pd.read_excel(io.BytesIO(file_resp.content),
                                    header=None,
                                    nrows=30)

            best_idx = 0
            max_score = 0
            keywords = [
                "nitrosamine", "limit", "intake", "substance", "ng/day",
                "iupac", "impurity", "structure", "cas", "source",
                "ai (ng/day)"
            ]

            for idx, row in temp_df.iterrows():
                row_text = " ".join(
                    [str(x).lower() for x in row if pd.notna(x)])
                score = sum(1 for k in keywords if k in row_text)

                if score > max_score:
                    max_score = score
                    best_idx = idx

            log_messages.append(
                f"Header Scoring: Selected Row {best_idx} with score {max_score}"
            )

            df = pd.read_excel(io.BytesIO(file_resp.content), header=best_idx)
            df.columns = [
                str(c).strip().replace('\n', ' ') for c in df.columns
            ]

            return df, log_messages
        return pd.DataFrame(), ["No link found"]
    except Exception as e:
        return pd.DataFrame(), [str(e)]


# ==========================================
# 3. æ ¸å¿ƒæ¯”å°é‚è¼¯ (Smart Match)
# ==========================================
def smart_match(scino_api, row_series):
    scino_clean = scino_api.upper().replace("-", " ").strip()
    scino_tokens = set(scino_clean.split())
    core_tokens = {
        t
        for t in scino_tokens if t not in STOP_WORDS and len(t) > 2
    }

    if not core_tokens:
        if "COMPOUND" in scino_clean:
            return False, ""
        core_tokens = {scino_clean}

    row_text = " ".join(
        [str(val).upper() for val in row_series.values if pd.notna(val)])

    for token in core_tokens:
        pattern = r'\b' + re.escape(token) + r'\b'
        if re.search(pattern, row_text):
            return True, row_text

    return False, ""


def get_display_col(df_columns, keyword_list):
    if isinstance(keyword_list, str):
        keyword_list = [keyword_list]

    cols = {c.lower(): c for c in df_columns}

    for kw in keyword_list:
        kw = kw.lower()
        if kw == 'name':
            for c_lower, c_orig in cols.items():
                if c_lower == 'name':
                    return c_orig

        for c_lower, c_orig in cols.items():
            if kw in c_lower:
                return c_orig
    return None


# ==========================================
# 4. Excel ç”Ÿæˆ
# ==========================================
def generate_excel(match_df, fda_raw, ema_raw):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        match_df.to_excel(writer, sheet_name='Summary_Match', index=False)
        fda_raw.to_excel(writer, sheet_name='Raw_FDA_Data', index=False)
        ema_raw.to_excel(writer, sheet_name='Raw_EMA_Data', index=False)

        workbook = writer.book
        for sheet in writer.sheets.values():
            sheet.set_column(0, 9, 20)

    return output.getvalue()


# ==========================================
# ä¸»ç¨‹å¼ UI
# ==========================================

# --- Sidebar: é¸æ“‡è³‡æ–™ä¾†æº ---
st.sidebar.header("âš™ï¸ è¨­å®š (Settings)")
source_mode = st.sidebar.radio(
    "é¸æ“‡ç”¢å“æ¸…å–®ä¾†æº (Source):",
    ("ğŸŒ è‡ªå‹•çˆ¬å–ç¥éš†å®˜ç¶² (Auto-Scrape)", "ğŸ“‚ æ‰‹å‹•ä¸Šå‚³æ¸…å–® (Manual Upload)"))

api_list = []
log_msgs = []
ready_to_run = False

if source_mode == "ğŸŒ è‡ªå‹•çˆ¬å–ç¥éš†å®˜ç¶² (Auto-Scrape)":
    st.sidebar.info("ç¨‹å¼å°‡è‡ªå‹•é€£ç·šè‡³ scinopharm.com ä¸‹è¼‰æœ€æ–°çš„ PDF ç”¢å“åˆ—è¡¨ã€‚")
    if st.sidebar.button("è¼‰å…¥å®˜ç¶²è³‡æ–™", type="primary"):
        with st.spinner("æ­£åœ¨é€£ç·šè‡³ç¥éš†å®˜ç¶²..."):
            api_list, log_msgs = get_scinopharm_apis_auto()
            if api_list:
                st.session_state['api_list'] = api_list
                st.session_state['log_msgs'] = log_msgs
                st.success(f"æˆåŠŸè¼‰å…¥ {len(api_list)} ç­†ç”¢å“ï¼")
            else:
                st.error("æœªæ‰¾åˆ°ç”¢å“ï¼Œè«‹æª¢æŸ¥é€£ç·šæˆ–æ”¹ç”¨æ‰‹å‹•ä¸Šå‚³ã€‚")

    if 'api_list' in st.session_state and st.session_state['api_list']:
        api_list = st.session_state['api_list']
        log_msgs = st.session_state['log_msgs']
        ready_to_run = True

else:
    st.sidebar.info("è«‹ä¸Šå‚³ Excel (.xlsx) æˆ– CSV æª”ã€‚æ”¯æ´ 'SPT' æ¬„ä½è‡ªå‹•è®€å–ã€‚")
    uploaded_file = st.sidebar.file_uploader("ä¸Šå‚³æª”æ¡ˆ", type=['xlsx', 'csv'])

    if uploaded_file:
        api_list, log_msgs = parse_uploaded_file(uploaded_file)
        if api_list:
            st.sidebar.success(f"âœ… å·²è®€å– {len(api_list)} ç­†è³‡æ–™")
            ready_to_run = True
            with st.expander("é è¦½åŒ¯å…¥æ¸…å–® (å‰ 5 ç­†)"):
                st.write(api_list[:5])
        else:
            st.sidebar.error("âŒ ç„¡æ³•è®€å–è³‡æ–™ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆæ ¼å¼ã€‚")

# --- ä¸»ç•«é¢ ---

if ready_to_run:
    st.subheader(
        f"ç›®å‰ç›£æ§æ¸…å–®: {len(api_list)} é …ç”¢å“ ({'è‡ªå‹•çˆ¬å–' if source_mode.startswith('ğŸŒ') else 'æ‰‹å‹•åŒ¯å…¥'})"
    )

    if st.button("ğŸš€ é–‹å§‹åŸ·è¡Œæ¯”å° (Start Analysis)", type="primary"):
        status_box = st.status("æ­£åœ¨åˆ†æä¸­...", expanded=True)

        # 2. FDA / EMA
        status_box.write("ğŸŒ ä¸‹è¼‰ FDA / EMA è³‡æ–™åº«...")
        fda_df, fda_logs = get_fda_data()
        ema_df, ema_logs = get_ema_data()

        if not fda_df.empty:
            status_box.write(
                f"âœ… FDA: {len(fda_df)} ç­† (å·²éæ¿¾åƒ… Table 1 & 2), EMA: {len(ema_df)} ç­†"
            )
        else:
            status_box.write(f"âš ï¸ FDA: 0 ç­† (æŠ“å–å¤±æ•—), EMA: {len(ema_df)} ç­†")
            log_msgs.extend(fda_logs)  # å°‡ FDA éŒ¯èª¤è¨Šæ¯åŠ å…¥ Log

        # 3. æ¯”å°
        status_box.write("ğŸ” åŸ·è¡Œæ¯”å°...")
        match_results = []

        # --- FDA æ¯”å° ---
        if not fda_df.empty:
            # v7.6: ä½¿ç”¨æ¨™æº–åŒ–å¾Œçš„æ¬„ä½åç¨±
            nitro_col = 'Nitrosamine' if 'Nitrosamine' in fda_df.columns else None
            limit_col = 'Limit' if 'Limit' in fda_df.columns else None
            iupac_col = 'IUPAC' if 'IUPAC' in fda_df.columns else None
            source_col = 'Source' if 'Source' in fda_df.columns else None
            note_col = 'Notes' if 'Notes' in fda_df.columns else None

            ref_col = source_col

            for _, row in fda_df.iterrows():
                for my_api_obj in api_list:
                    my_api_name = my_api_obj['name']
                    my_api_spt = my_api_obj['spt']

                    is_match, _ = smart_match(my_api_name, row)
                    if is_match:
                        match_results.append({
                            "Source":
                            "USFDA",
                            "ScinoPharm Product":
                            my_api_name,
                            "SPT Project num":
                            my_api_spt,
                            "Nitrosamine Impurity":
                            row[nitro_col] if nitro_col
                            and pd.notna(row[nitro_col]) else "Check Row",
                            "IUPAC Name":
                            row[iupac_col] if iupac_col
                            and pd.notna(row[iupac_col]) else "N/A",
                            "Limit (AI)":
                            row[limit_col] if limit_col
                            and pd.notna(row[limit_col]) else "N/A",
                            "Notes":
                            row[note_col]
                            if note_col and pd.notna(row[note_col]) else "N/A",
                            "Matched in Column":
                            "Full Row",
                            "Reference Value":
                            row[ref_col] if ref_col and pd.notna(row[ref_col])
                            else "See Raw Data"
                        })

        # --- EMA æ¯”å° ---
        if not ema_df.empty:
            # EMA æ¬„ä½å°‹æ‰¾é‚è¼¯ç¶­æŒå‹•æ…‹ï¼Œå› ç‚ºåªæœ‰ä¸€å€‹ Excel
            cols = {c.lower(): c for c in ema_df.columns}
            nitro_col = next((cols[c] for c in cols if any(
                x in c for x in ['name', 'nitrosamine', 'impurity'])), None)
            limit_col = next((cols[c] for c in cols if any(
                x in c for x in ['ai (ng/day)', 'limit', 'intake', 'ai'])),
                             None)
            iupac_col = next(
                (cols[c]
                 for c in cols if any(x in c
                                      for x in ['iupac', 'chemical name'])),
                None)
            source_col = next((cols[c] for c in cols if 'source' in c), None)
            drug_col = next((cols[c] for c in cols if any(
                x in c for x in ['substance', 'api', 'product', 'active'])),
                            None)
            note_col = next(
                (cols[c]
                 for c in cols if any(x in c
                                      for x in ['note', 'comment', 'remark'])),
                None)
            ref_col = source_col if source_col else drug_col

            for _, row in ema_df.iterrows():
                for my_api_obj in api_list:
                    my_api_name = my_api_obj['name']
                    my_api_spt = my_api_obj['spt']

                    is_match, _ = smart_match(my_api_name, row)
                    if is_match:
                        match_results.append({
                            "Source":
                            "EMA",
                            "ScinoPharm Product":
                            my_api_name,
                            "SPT Project num":
                            my_api_spt,
                            "Nitrosamine Impurity":
                            row[nitro_col] if nitro_col
                            and pd.notna(row[nitro_col]) else "Check Row",
                            "IUPAC Name":
                            row[iupac_col] if iupac_col
                            and pd.notna(row[iupac_col]) else "N/A",
                            "Limit (AI)":
                            row[limit_col] if limit_col else "N/A",
                            "Notes":
                            row[note_col]
                            if note_col and pd.notna(row[note_col]) else "N/A",
                            "Matched in Column":
                            ref_col if ref_col else "Full Row Match",
                            "Reference Value":
                            row[ref_col] if ref_col else "See Raw Data"
                        })

        status_box.update(label="åŸ·è¡Œå®Œæˆï¼", state="complete", expanded=False)

        # --- çµæœé¡¯ç¤º ---
        st.divider()

        if match_results:
            final_df = pd.DataFrame(match_results).drop_duplicates()

            # èª¿æ•´æ¬„ä½é †åº
            cols_order = [
                "Source", "ScinoPharm Product", "SPT Project num",
                "Nitrosamine Impurity", "IUPAC Name", "Limit (AI)", "Notes",
                "Reference Value"
            ]
            cols_order = [c for c in cols_order if c in final_df.columns]
            final_df = final_df[cols_order]

            st.subheader(f"ğŸ“Š æ¯”å°çµæœ (å…± {len(final_df)} ç­†)")
            st.dataframe(final_df, use_container_width=True, height=500)

            excel_data = generate_excel(final_df, fda_df, ema_df)
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰å®Œæ•´ Excel å ±è¡¨",
                data=excel_data,
                file_name='ScinoPharm_Nitrosamine_Analysis_v7.6.xlsx',
                mime=
                'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                type="primary")
        else:
            st.warning("âš ï¸ æ²’æœ‰æ¯”å°åˆ°çµæœã€‚")
else:
    st.info("ğŸ‘ˆ è«‹åœ¨å·¦å´å´é‚Šæ¬„é¸æ“‡è³‡æ–™ä¾†æºä¸¦è¼‰å…¥è³‡æ–™ã€‚")

# --- Debug Logs ---
with st.expander("ğŸ› ï¸ Debug Logs"):
    if log_msgs:
        for msg in log_msgs:
            st.text(msg)
    else:
        st.text("å°šç„¡ç´€éŒ„")

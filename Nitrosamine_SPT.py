import streamlit as st
import pandas as pd
import requests
import pdfplumber
import io
import re
import warnings
from bs4 import BeautifulSoup
import urllib3

# å¿½ç•¥ SSL è­¦å‘Š
warnings.filterwarnings("ignore")
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="ScinoPharm Nitrosamine Monitor (Fixed)",
                   layout="wide")
st.title("ğŸ§ª ScinoPharm Nitrosamine Monitor")
st.markdown("æ­¤å·¥å…·ç”¨æ–¼è§£æç¥éš†è—¥å“æ¸…å–® PDFï¼Œä¸¦æå– API åç¨±èˆ‡ç›¸é—œè³‡è¨Šã€‚")

# ==========================================
# 0. å®šç¾©é€šç”¨å­—èˆ‡é›œè¨Š (Stop Words)
# ==========================================
STOP_WORDS = {
    "ACID", "SODIUM", "POTASSIUM", "CALCIUM", "MAGNESIUM", "HYDROCHLORIDE",
    "HCL", "HYDROBROMIDE", "HBR", "ACETATE", "TARTRATE", "CITRATE", "MALEATE",
    "FUMARATE", "MESYLATE", "SUCCINATE", "PHOSPHATE", "SULFATE", "BASE", "USP",
    "EP", "BP", "JP", "TABLETS", "CAPSULES", "INJECTION", "SOLUTION", "ORAL",
    "EXTENDED", "RELEASE", "API", "NAME", "PRODUCT", "DRUG", "SUBSTANCE",
    "UNKNOWN", "AND", "WITH"
}


# ==========================================
# 1. æ ¸å¿ƒå‡½æ•¸: ç¥éš† PDF è§£æ
# ==========================================
@st.cache_data(ttl=3600)
def get_scinopharm_apis():
    base_url = "https://www.scinopharm.com"
    target_url = "https://www.scinopharm.com/tw/products-detail/commercialAPI/"

    REAL_HEADERS = {
        "User-Agent":
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept":
        "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Referer": "https://www.scinopharm.com/"
    }

    product_list = set()
    debug_logs = []

    try:
        r = requests.get(target_url,
                         headers=REAL_HEADERS,
                         verify=False,
                         timeout=15)
        soup = BeautifulSoup(r.text, 'html.parser')

        target_keywords = ["ä¸‹è¼‰ç”¢å“æ¸…å–®", "ä¸‹è¼‰è—¥ç‰©ä¸»æª”ç”³è«‹åˆ—è¡¨"]
        pdf_links = []

        for a in soup.find_all('a', href=True):
            if any(k in a.get_text(strip=True) for k in target_keywords):
                full_link = a['href']
                if not full_link.startswith("http"):
                    full_link = base_url + full_link if full_link.startswith(
                        "/") else base_url + "/" + full_link
                pdf_links.append(full_link)

        if not pdf_links:
            pdf_links.append("https://www.scinopharm.com/tw/download/43/")
            debug_logs.append("âš ï¸ æœªåœ¨é é¢æ‰¾åˆ°é€£çµï¼Œä½¿ç”¨é è¨­ ID 43 é€²è¡Œå˜—è©¦ã€‚")

        for link in pdf_links:
            debug_logs.append(f"è™•ç†é€£çµ: {link}")
            try:
                pdf_resp = requests.get(link,
                                        headers=REAL_HEADERS,
                                        verify=False,
                                        timeout=15)
                pdf_resp.raise_for_status()

                if not pdf_resp.content.startswith(b'%PDF-'):
                    debug_logs.append(f"âŒ ç•¥é: ä¸‹è¼‰å…§å®¹ä¸æ˜¯ PDF (å¯èƒ½æ˜¯ HTML éŒ¯èª¤é é¢)ã€‚")
                    continue

                debug_logs.append("âœ… æ ¼å¼é©—è­‰æˆåŠŸï¼Œé–‹å§‹è§£æ...")

                with pdfplumber.open(io.BytesIO(pdf_resp.content)) as pdf:
                    for page in pdf.pages:
                        tables = page.extract_tables()
                        found_in_table = False
                        if tables:
                            for table in tables:
                                for row in table:
                                    if row and len(row) > 0:
                                        val = str(row[0]).strip()
                                        if is_valid_api_name(val):
                                            product_list.add(
                                                clean_api_name(val))
                                            found_in_table = True

                        if not found_in_table:
                            text = page.extract_text()
                            if text:
                                lines = text.split('\n')
                                for line in lines:
                                    parts = re.split(r'\s{2,}', line.strip())
                                    if parts:
                                        candidate = parts[0]
                                        if is_valid_api_name(candidate):
                                            product_list.add(
                                                clean_api_name(candidate))

            except requests.exceptions.RequestException as e:
                debug_logs.append(f"âŒ ç¶²è·¯è«‹æ±‚å¤±æ•—: {e}")
            except Exception as e:
                debug_logs.append(f"âŒ è§£æéç¨‹éŒ¯èª¤: {e}")

    except Exception as e:
        debug_logs.append(f"âŒ åˆå§‹é€£ç·šå¤±æ•—: {e}")

    return sorted(list(product_list)), debug_logs


def is_valid_api_name(text):
    if not text: return False
    text = text.lower()
    ignore = [
        "api name", "regulatory", "therapeutic", "page", "scinopharm",
        "download", "date", "status", "product"
    ]
    if any(x in text for x in ignore): return False
    if len(text) < 3: return False
    if not re.search(r'[a-zA-Z]', text): return False
    return True


def clean_api_name(text):
    text = re.sub(r'\s*\(.*?\)', '', text)
    text = text.replace('Â®', '').replace('â„¢', '').replace('*', '')
    return text.strip()


# ==========================================
# 2. çˆ¬èŸ²å‡½æ•¸: USFDA & EMA
# ==========================================
@st.cache_data(ttl=86400)
def get_fda_data():
    url = "https://www.fda.gov/regulatory-information/search-fda-guidance-documents/cder-nitrosamine-impurity-acceptable-intake-limits"
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=headers, verify=False)
        dfs = pd.read_html(io.StringIO(r.text))

        # åˆä½µæ‰€æœ‰è¡¨æ ¼ (v5.8 é‚è¼¯ä¿æŒ)
        valid_dfs = []
        for df in dfs:
            df.columns = [
                str(c).strip().replace('\n', ' ') for c in df.columns
            ]
            headers_str = " ".join([c.lower() for c in df.columns])
            if "nitrosamine" in headers_str or "limit" in headers_str or "ai" in headers_str:
                valid_dfs.append(df)

        if valid_dfs:
            final_df = pd.concat(valid_dfs, ignore_index=True)
            return final_df

        return pd.DataFrame()
    except Exception as e:
        return pd.DataFrame()


@st.cache_data(ttl=86400)
def get_ema_data():
    base_url = "https://www.ema.europa.eu"
    page_url = "https://www.ema.europa.eu/en/human-regulatory-overview/post-authorisation/pharmacovigilance-post-authorisation/referral-procedures-human-medicines/nitrosamine-impurities/nitrosamine-impurities-guidance-marketing-authorisation-holders"

    log_messages = []

    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
        r = requests.get(page_url, headers=headers, verify=False)
        soup = BeautifulSoup(r.text, 'html.parser')

        target_link = None
        for a in soup.find_all('a', href=True):
            href = a['href']
            text = a.get_text(strip=True).lower()
            if "xlsx" in href and ("appendix" in text or "limit" in text):
                target_link = href
                break

        if not target_link:
            for a in soup.find_all('a', href=True):
                if "xlsx" in a['href']:
                    target_link = a['href']
                    break

        if target_link:
            if not target_link.startswith("http"):
                target_link = base_url + target_link

            file_resp = requests.get(target_link,
                                     headers=headers,
                                     verify=False)

            temp_df = pd.read_excel(io.BytesIO(file_resp.content),
                                    header=None,
                                    nrows=30)

            best_idx = 0
            max_score = 0
            keywords = [
                "nitrosamine", "limit", "intake", "substance", "ng/day",
                "iupac", "impurity", "structure", "cas", "source",
                "ai (ng/day)"
            ]

            for idx, row in temp_df.iterrows():
                row_text = " ".join(
                    [str(x).lower() for x in row if pd.notna(x)])
                score = sum(1 for k in keywords if k in row_text)

                if score > max_score:
                    max_score = score
                    best_idx = idx

            log_messages.append(
                f"Header Scoring: Selected Row {best_idx} with score {max_score}"
            )

            df = pd.read_excel(io.BytesIO(file_resp.content), header=best_idx)
            df.columns = [
                str(c).strip().replace('\n', ' ') for c in df.columns
            ]

            return df, log_messages
        return pd.DataFrame(), ["No link found"]
    except Exception as e:
        return pd.DataFrame(), [str(e)]


# ==========================================
# 3. æ ¸å¿ƒæ¯”å°é‚è¼¯ (Smart Match)
# ==========================================
def smart_match(scino_api, row_series):
    scino_clean = scino_api.upper().replace("-", " ").strip()
    scino_tokens = set(scino_clean.split())
    core_tokens = {
        t
        for t in scino_tokens if t not in STOP_WORDS and len(t) > 2
    }

    if not core_tokens:
        core_tokens = {scino_clean}

    row_text = " ".join(
        [str(val).upper() for val in row_series.values if pd.notna(val)])

    for token in core_tokens:
        if token in row_text:
            return True, row_text

    return False, ""


def get_display_col(df_columns, keyword_list):
    if isinstance(keyword_list, str):
        keyword_list = [keyword_list]

    cols = {c.lower(): c for c in df_columns}

    for kw in keyword_list:
        kw = kw.lower()
        if kw == 'name':
            for c_lower, c_orig in cols.items():
                if c_lower == 'name':
                    return c_orig

        for c_lower, c_orig in cols.items():
            if kw in c_lower:
                return c_orig
    return None


# ==========================================
# 4. Excel ç”Ÿæˆ
# ==========================================
def generate_excel(match_df, fda_raw, ema_raw):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        match_df.to_excel(writer, sheet_name='Summary_Match', index=False)
        fda_raw.to_excel(writer, sheet_name='Raw_FDA_Data', index=False)
        ema_raw.to_excel(writer, sheet_name='Raw_EMA_Data', index=False)

        workbook = writer.book
        for sheet in writer.sheets.values():
            sheet.set_column(0, 8, 20)

    return output.getvalue()


# ==========================================
# ä¸»ç¨‹å¼ UI
# ==========================================
if st.button("ğŸš€ åŸ·è¡Œæœ€å¤§åŒ–æ¯”å° ", type="primary"):

    status_box = st.status("æ­£åœ¨è™•ç†ä¸­...", expanded=True)

    # 1. ScinoPharm
    status_box.write("ğŸ“¥ ä¸‹è¼‰ç¥éš† PDF...")
    scino_apis, scino_logs = get_scinopharm_apis()

    if len(scino_apis) > 0:
        status_box.write(f"âœ… ç¥éš† API: {len(scino_apis)} ç­†")

    # 2. FDA / EMA
    status_box.write("ğŸŒ ä¸‹è¼‰ FDA / EMA è³‡æ–™åº«...")
    fda_df = get_fda_data()
    ema_df, ema_logs = get_ema_data()
    status_box.write(f"âœ… FDA: {len(fda_df)} ç­†, EMA: {len(ema_df)} ç­†")

    # 3. æ¯”å°
    status_box.write("ğŸ” åŸ·è¡Œæ¯”å°...")
    match_results = []

    # --- FDA æ¯”å° ---
    if not fda_df.empty:
        nitro_col = get_display_col(fda_df.columns, 'nitrosamine')
        limit_col = get_display_col(fda_df.columns,
                                    ['limit', 'intake', 'ng/day'])
        iupac_col = get_display_col(fda_df.columns, ['iupac', 'chemical name'])
        source_col = get_display_col(fda_df.columns, 'source')
        drug_col = get_display_col(fda_df.columns, 'drug')

        # ã€ä¿®æ­£ã€‘åªæŠ“ Notesï¼Œç§»é™¤ Surrogate
        note_col = get_display_col(fda_df.columns,
                                   ['note', 'comment', 'remark'])

        ref_col = source_col if source_col else drug_col

        for _, row in fda_df.iterrows():
            for my_api in scino_apis:
                is_match, _ = smart_match(my_api, row)
                if is_match:
                    match_results.append({
                        "Source":
                        "USFDA",
                        "ScinoPharm Product":
                        my_api,
                        "Nitrosamine Impurity":
                        row[nitro_col] if nitro_col else "Check Row",
                        "IUPAC Name":
                        row[iupac_col] if iupac_col else "N/A",
                        "Limit (AI)":
                        row[limit_col] if limit_col else "N/A",
                        "Notes":
                        row[note_col] if note_col else "N/A",  # åªé¡¯ç¤º Notes
                        "Matched in Column":
                        ref_col if ref_col else "Full Row Match",
                        "Reference Value":
                        row[ref_col] if ref_col else "See Raw Data"
                    })

    # --- EMA æ¯”å° ---
    if not ema_df.empty:
        nitro_col = get_display_col(ema_df.columns,
                                    ['name', 'nitrosamine', 'impurity'])
        limit_col = get_display_col(ema_df.columns,
                                    ['ai (ng/day)', 'limit', 'intake', 'ai'])
        iupac_col = get_display_col(ema_df.columns, ['iupac', 'chemical name'])
        source_col = get_display_col(ema_df.columns, 'source')
        drug_col = get_display_col(ema_df.columns,
                                   ['substance', 'api', 'product', 'active'])

        # ã€ä¿®æ­£ã€‘åªæŠ“ Notesï¼Œç§»é™¤ Surrogate
        note_col = get_display_col(ema_df.columns,
                                   ['note', 'comment', 'remark'])

        ref_col = source_col if source_col else drug_col

        for _, row in ema_df.iterrows():
            for my_api in scino_apis:
                is_match, _ = smart_match(my_api, row)
                if is_match:
                    match_results.append({
                        "Source":
                        "EMA",
                        "ScinoPharm Product":
                        my_api,
                        "Nitrosamine Impurity":
                        row[nitro_col] if nitro_col
                        and pd.notna(row[nitro_col]) else "Check Row",
                        "IUPAC Name":
                        row[iupac_col]
                        if iupac_col and pd.notna(row[iupac_col]) else "N/A",
                        "Limit (AI)":
                        row[limit_col] if limit_col else "N/A",
                        "Notes":
                        row[note_col] if note_col and pd.notna(row[note_col])
                        else "N/A",  # åªé¡¯ç¤º Notes
                        "Matched in Column":
                        ref_col if ref_col else "Full Row Match",
                        "Reference Value":
                        row[ref_col] if ref_col else "See Raw Data"
                    })

    status_box.update(label="åŸ·è¡Œå®Œæˆï¼", state="complete", expanded=False)

    # --- çµæœé¡¯ç¤º ---
    st.divider()

    if match_results:
        final_df = pd.DataFrame(match_results).drop_duplicates()

        # èª¿æ•´æ¬„ä½é †åº (æ”¹ç‚º Notes)
        cols_order = [
            "Source", "ScinoPharm Product", "Nitrosamine Impurity",
            "IUPAC Name", "Limit (AI)", "Notes", "Reference Value"
        ]
        cols_order = [c for c in cols_order if c in final_df.columns]
        final_df = final_df[cols_order]

        st.subheader(f"ğŸ“Š æ¯”å°çµæœ (å…± {len(final_df)} ç­†)")
        st.dataframe(final_df, use_container_width=True, height=500)

        excel_data = generate_excel(final_df, fda_df, ema_df)
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰å®Œæ•´ Excel å ±è¡¨",
            data=excel_data,
            file_name='ScinoPharm_Nitrosamine_Analysis_v5.9.xlsx',
            mime=
            'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            type="primary")
    else:
        st.warning("âš ï¸ æ²’æœ‰æ¯”å°åˆ°çµæœã€‚")

    # --- Debug Logs å€å¡Š ---
    with st.expander("ğŸ› ï¸ Debug Logs (EMA æ¬„ä½æª¢æŸ¥)"):
        st.info(f"ç¥éš†ç”¢å“æ•¸: {len(scino_apis)}")

        st.markdown("---")
        if not fda_df.empty:
            st.write("ğŸ” FDA Detected Columns (After Merge):")
            st.write(fda_df.columns.tolist())
            st.write(
                f"- Note Col: {get_display_col(fda_df.columns, ['note', 'comment', 'remark'])}"
            )

        st.markdown("---")
        if not ema_df.empty:
            st.write("ğŸ” EMA Detected Columns:")
            st.write(
                f"- Nitrosamine Col: {get_display_col(ema_df.columns, ['name', 'nitrosamine', 'impurity'])}"
            )
            st.write(
                f"- Limit Col: {get_display_col(ema_df.columns, ['ai (ng/day)', 'limit', 'intake', 'ai'])}"
            )
            st.write(
                f"- IUPAC Col: {get_display_col(ema_df.columns, ['iupac', 'chemical name'])}"
            )
            st.write(
                f"- Note Col: {get_display_col(ema_df.columns, ['note', 'comment', 'remark'])}"
            )
        else:
            st.error("âš ï¸ EMA è³‡æ–™æœªè¼‰å…¥")
